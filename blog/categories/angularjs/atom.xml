<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Angularjs | :baopham]]></title>
  <link href="http://baopham.github.io/blog/categories/angularjs/atom.xml" rel="self"/>
  <link href="http://baopham.github.io/"/>
  <updated>2016-05-17T01:52:48-07:00</updated>
  <id>http://baopham.github.io/</id>
  <author>
    <name><![CDATA[Bao Pham]]></name>
    <email><![CDATA[gbaopham@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Laravel + Angularjs: file upload pattern]]></title>
    <link href="http://baopham.github.io/blog/2015/03/22/laravel-plus-angularjs-file-upload-pattern/"/>
    <updated>2015-03-22T22:36:38-07:00</updated>
    <id>http://baopham.github.io/blog/2015/03/22/laravel-plus-angularjs-file-upload-pattern</id>
    <content type="html"><![CDATA[<p>The thing with uploading files via AJAX is that users can just keep uploading files to the server without having to save the form. This leaves a lot of junk files around if you don&rsquo;t clean them up later. But we don&rsquo;t want to delete the files that are actually being used. This means we need to keep records of uploaded files.</p>

<p>This pattern is working for me &ndash; assuming we want to keep files on S3:</p>

<!--more-->


<p><strong>1.</strong> A table to store records of uploaded files.</p>

<p>```php
// database/migrations/2015_03_05_004850_create_files_table.php</p>

<p>public function up()
{</p>

<pre><code>Schema::create('files', function(Blueprint $table)
{
    $table-&gt;string('id', 36)-&gt;primary();
    $table-&gt;string('mime', 255);
    $table-&gt;string('filename', 255);
    $table-&gt;bigInteger('size')-&gt;unsigned();
    $table-&gt;string('storage_path')-&gt;unique();
    $table-&gt;string('disk', 10);
    $table-&gt;boolean('status');
    $table-&gt;timestamps();
});
</code></pre>

<p>}</p>

<p>// app/Models/File.php</p>

<p>use CRT\Models\BaseModel;
use Illuminate\Support\Facades\Storage;</p>

<p>class File extends BaseModel {</p>

<pre><code>protected $fillable = ['mime', 'storage_path', 'status', 'filename', 'size', 'disk'];

public static function boot()
{
    parent::boot();

    static::deleting(function($model)
    {
        Storage::disk($model-&gt;disk)-&gt;delete($model-&gt;storage_path);
    });
}
</code></pre>

<p>}
```</p>

<p><strong>2.</strong> Use Javascript library to upload files thru AJAX (<a href="https://github.com/flowjs">Flowjs</a>, <a href="http://www.dropzonejs.com/">Dropzonejs</a>, etc.)</p>

<p><strong>3.</strong> In the controller&rsquo;s upload method:</p>

<p>```php
// app/Http/Controllers/FileUploadController.php</p>

<p>use CRT\Models\File as FileModel;</p>

<p>use Flow\Config as FlowConfig;
use Flow\Request as FlowRequest;
use Flow\ConfigInterface;
use Flow\RequestInterface;</p>

<p>public function upload()
{</p>

<pre><code>$config = new FlowConfig();
$config-&gt;setTempDir(storage_path() . '/tmp');
$config-&gt;setDeleteChunksOnSave(false);

$request = new FlowRequest();

$totalSize = $request-&gt;getTotalSize();

if ($totalSize &amp;&amp; $totalSize &gt; (1024 * 1024 * 4))
{
    return $this-&gt;responseWithError('File size exceeds 4MB');
}

$uploadFile = $request-&gt;getFile();

list($path, $destination) = $this-&gt;getDestinationPathInfo($uploadFile);

// @see: http://baopham.github.io/blog/2015/03/16/laravel-merge-flowjs-chunks-directly-to-s3
if (\Bao\Flow\Basic::save($destination, $config, $request, $this-&gt;s3Client))
{
    $file = FileModel::create([
        'mime' =&gt; $uploadFile['type'],
        'size' =&gt; $request-&gt;getTotalSize(),
        'storage_path' =&gt; $path,
        'filename' =&gt; $uploadFile['name'],
        'disk' =&gt; 's3',
        'status' =&gt; false,
    ]);

    // Send JSON response.
    return $this-&gt;responseWithData($file);
}
else
{
    // Indicate that we are not done with all the chunks.
    return $this-&gt;responseWithData('', 204);
}
</code></pre>

<p>}
```
This will save the <a href="http://baopham.github.io/blog/2015/03/16/laravel-merge-flowjs-chunks-directly-to-s3/">uploaded file to S3 directly</a>.</p>

<p><strong>4.</strong> Angular side will store the response that has the file&rsquo;s info (id, size, filename, etc.) &ndash; mark this file as new, e.g:</p>

<p>```javascript
// public/assets/js/thing/controller.js</p>

<p>var vm = this;
vm.processFileSuccess = processFileSuccess;</p>

<p>// &hellip;</p>

<p>function processFileSuccess(file) {</p>

<pre><code>file.isNew = true;
vm.files.push(file);
</code></pre>

<p>}
```</p>

<p><strong>5.</strong> When user saves the form, we submit the form including the file&rsquo;s info.</p>

<p><strong>6.</strong> Then in the backend, we process the file and mark the file&rsquo;s status as <code>true</code>:</p>

<p>```php
$store = Input::all();
foreach ($store[&lsquo;files&rsquo;] as &amp;$fileInput)
{</p>

<pre><code>if (array_get($fileInput, 'isNew'))
{
    $file = FileModel::find($fileInput['id']);
    if ($file-&gt;disk == 's3')
    {
        // move file to a different S3 folder
        ...
        // update file's storage path and status
        $file-&gt;storage_path = ...;
        $file-&gt;status = true;
        $file-&gt;save();
    }
    else
    {
        // upload local file to S3 then update the file's record if needed or just delete it:
        $file-&gt;delete();
    }
    // update the file input info before saving it to DB - store the S3 URL for example:
    $fileInput['url'] = ...;
}
</code></pre>

<p>}</p>

<p>// then save $store in DB.
```</p>

<p><strong>7.</strong> Create a command to clean up tmp files that are older than x hours: <code>php artisan make:console CleanupFiles</code></p>

<p>```php
// app/Console/Commands/CleanupFiles.php</p>

<p>/<em>*
 * The console command name.
 *
 * @var string
 </em>/
protected $name = &lsquo;files:clean&rsquo;;</p>

<p>/<em>*
 * Execute the console command.
 *
 * @return mixed
 </em>/
public function fire()
{</p>

<pre><code>$date = new DateTime;
$date-&gt;modify('-3 hours');
$formatted = $date-&gt;format('Y-m-d H:i:s');
FileModel::where('updated_at', '&lt;=', $formatted)
    -&gt;where('status', '=', 0)
    -&gt;get()-&gt;each(function($file)
    {
        $file-&gt;delete();
    });

\Flow\Uploader::pruneChunks(config('filesystems.disks.local.root') . '/tmp', 3*60*60);
$this-&gt;info('Files older than 3 hours have been cleaned up.');
</code></pre>

<p>}
```</p>

<p>Set a scheduler for it:</p>

<p>```php
// app/Console/Kernel.php</p>

<p>protected $commands = [</p>

<pre><code>'CRT\Console\Commands\CleanupFiles',
</code></pre>

<p>];</p>

<p>protected function schedule(Schedule $schedule)
{</p>

<pre><code>$schedule-&gt;command('files:clean')
        -&gt;hourly();
</code></pre>

<p>}
```</p>

<p>And then setup cronjob:</p>

<p><code>
* * * * * php /home/vagrant/projects/CRT/artisan schedule:run 1&gt;&gt; /dev/null 2&gt;&amp;1
</code></p>
]]></content>
  </entry>
  
</feed>
